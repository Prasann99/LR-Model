# -*- coding: utf-8 -*-
"""Prasann_Patel_LAB2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eDWCRRI5uOH3wHuXsaFX64m8nbvO4z6R
"""

import sys 
import numpy as np
from scipy.stats import randint
import pandas as pd  
import matplotlib.pyplot as plt  
import seaborn as sns  

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier, LocalOutlierFactor,KNeighborsClassifier, NeighborhoodComponentsAnalysis
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report,accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split, KFold,GridSearchCV

data = pd.read_csv("dataset.csv")

data.head()

data.diagnosis.unique()

data[data == '?'] = np.NaN
data.info()

a = data.diagnosis 
list = ['id','diagnosis']
features = data.drop(list,axis = 1,inplace = False)

list = ['id']
data.drop(list, axis = 1, inplace = True)

print(data.isnull().sum())

B, M = data['diagnosis'].value_counts()
plt.figure(figsize=(7,4))
sns.set_context('notebook', font_scale=1)
sns.countplot(x = 'diagnosis',data=data)

features.describe()

data.keys()

stdX = (features - features.mean()) / (features.std())              
new_data = pd.concat([a,stdX.iloc[:,:]],axis=1)
new_data = pd.melt(new_data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')

plt.figure(figsize=(12,30))
sns.set_context('notebook', font_scale=1)
sns.boxenplot(x="value", y="features", hue="diagnosis", data=new_data)

new_data.head()

plt.figure(figsize=(12,30))
sns.set_context('notebook', font_scale=1)
sns.boxenplot(x="value", y="features", hue="diagnosis", data=new_data)



p =np.corrcoef(data['radius_mean'], data['perimeter_mean'])
s =np.corrcoef(data['radius_mean'], data['symmetry_mean'])
p1=p[0,1]
s1=s[0,1]
r = []
for (i,j) in zip(range(1,31),range(1,31)):
        q = np.corrcoef(data.iloc[:,1], data.iloc[:,j])
        q1= q[0,1]
        if abs(q1) >= 0.80 and data.columns[j]  not in r:
                    r.append(data.columns[j]) 
print()
print('* Lenght of columns assuming q >=0.80:', len(r)) 
print('r =',r)

r = []
for (i,j) in zip(range(1,31),range(1,31)):
        q = np.corrcoef(data.iloc[:,1], data.iloc[:,j])
        q1= q[0,1]
        if abs(q1) >= 0.40 and data.columns[j]  not in r:
                    r.append(data.columns[j]) 
print()
print('* Lenght of columns assuming q >=0.40:', len(r)) 
print('r =',r)









data1=data.copy()
data1['diagnosis']=data['diagnosis'].map({'M':1,'B':0})

X = data1.drop('diagnosis', axis=1)
y = data1['diagnosis']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, 
                                                    stratify=y)

print(type(X))
print(type(y))



#KNN
knn = KNeighborsClassifier()
parameters = {'n_neighbors': (range(1, 20)),'weights': [ 'distance', 'uniform']}  

grid_search = GridSearchCV(knn, parameters, cv=10) 
grid_search.fit(X_train, y_train)
#knn.fit(X_train, y_train)

print("Best Score is ", grid_search.best_score_)
print("Best Estinator is ", grid_search.best_estimator_)
print("Best Parametes are", grid_search.best_params_)

knn = KNeighborsClassifier(n_neighbors =  9, weights = 'uniform')
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
accuracy_knn = accuracy_score(y_test,y_pred)
print("Improved Accuracy on Test Data:",accuracy_knn)

#Randam Forest
Ran = RandomForestClassifier()
parameters = {'max_depth': [5,15,25], 
              'max_features': ['auto', 'sqrt'],
              'bootstrap': [True, False],
              }

ran = GridSearchCV(Ran, parameters, cv=10) 
ran.fit(X_train, y_train)
print("Best Score is ", ran.best_score_)
print("Best Estimator is ", ran.best_estimator_)

Ran = RandomForestClassifier( max_depth = 25, max_features = 'sqrt', n_estimators = 200)
Ran.fit(X_train, y_train)
y_pred = Ran.predict(X_test)
accuracy = accuracy_score(y_test,y_pred)
print("Improved Accuracy on Test Data:",accuracy)

#Descision Tree

tree = DecisionTreeClassifier()
parameters = {'min_samples_split': [2,3,4], 
              'min_samples_leaf':[2,3,4,5],
              'max_features': ['sqrt', 'auto']}


tree_cv = GridSearchCV(tree, parameters, cv=5)

tree_cv.fit(X_train, y_train)


print("Best Score is ",tree_cv.best_score_)
print("Best Estinator is ", tree_cv.best_estimator_)
print("Best Parametes are", tree_cv.best_params_)

tree = DecisionTreeClassifier(max_features = 'auto', min_samples_leaf=5, min_samples_split=2)
tree.fit(X_train, y_train)
y_pred = tree_cv.predict(X_test)
accuracy_dt = accuracy_score(y_test,y_pred)
print("Improved Accuracy on Test Data:",accuracy_dt)